{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b371dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#pip3 install bpmll\n",
    "#using bpmll in the loss\n",
    "from bpmll import bp_mll_loss\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "#pip install --upgrade gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76210a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8296ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the author out \n",
    "label_au = []\n",
    "for i,d in enumerate(train_data):\n",
    "    author = np.array(d['authors'])\n",
    "    author_100 = author[author<100]\n",
    "    #length is 101, put the author number which greater than 100 in the last one in the array \n",
    "    au_range = np.zeros(101)\n",
    "    if len(author_100) == 0:\n",
    "        au_range[-1] = 1 \n",
    "    else:\n",
    "        au_range[author_100] = 1\n",
    "    label_au.append(au_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518ac455",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_label_train = np.array(label_au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4a698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using word2vec\n",
    "title_l = []\n",
    "abstract_l = []\n",
    "for data in train_data:\n",
    "    title_l.append(data['title'])\n",
    "    abstract_l.append(data['abstract'])\n",
    "title_abst = title_l+abstract_l\n",
    "docum = [TaggedDocument(docm,[i]) for i , docm in enumerate(title_abst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80d2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = Doc2Vec(docum,vector_size = 256,window = 2,min_count =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7fc92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.22682028e-03, -1.19128090e-03, -1.24879624e-03, -4.72046435e-04,\n",
       "        7.03190686e-04,  1.93640031e-03,  1.08795310e-03,  1.35309761e-03,\n",
       "        3.93903349e-04,  5.04254596e-04,  1.41281774e-03, -2.56883213e-05,\n",
       "        1.28149311e-03,  4.29248903e-05,  1.70569820e-03,  1.42676290e-05,\n",
       "        1.88375614e-03, -7.70724495e-04,  8.41811998e-04,  1.94717525e-03,\n",
       "       -1.54935766e-03,  5.49636548e-04, -3.23833083e-04, -8.55718739e-04,\n",
       "       -1.91921019e-03,  3.82552622e-04, -8.73192330e-04, -1.03918510e-03,\n",
       "       -1.70134462e-03, -1.58110936e-03,  6.11537136e-04, -1.82162481e-03,\n",
       "       -1.75517623e-03,  1.02145458e-03, -1.18117128e-03, -1.14864856e-03,\n",
       "        2.87811970e-04, -1.69097004e-03, -1.74781273e-03, -7.17488583e-05,\n",
       "        1.50962314e-03, -1.52260275e-03,  5.03086485e-04,  1.01283239e-03,\n",
       "       -1.55485072e-03, -5.10125305e-04, -1.57363061e-03, -8.71573226e-04,\n",
       "       -1.01270224e-03, -1.61302625e-04,  1.10509433e-03, -1.31899258e-04,\n",
       "        1.35625387e-03,  7.54345208e-04,  9.33485106e-04,  3.93146882e-04,\n",
       "       -6.37015793e-04,  1.48578058e-03, -1.51107484e-03,  1.67758390e-03,\n",
       "       -6.00870000e-04, -4.53589950e-04, -4.30115731e-04,  1.07753230e-03,\n",
       "       -1.91067799e-03,  8.68253177e-04, -1.95261440e-03,  1.14712841e-03,\n",
       "       -1.63074676e-03, -1.02454075e-03, -4.46173945e-04,  1.38592697e-03,\n",
       "        4.63859411e-04, -2.97086895e-04,  1.01162354e-04,  1.74240721e-03,\n",
       "        1.63543923e-03, -1.19534635e-03, -9.89989145e-04,  1.78569811e-03,\n",
       "        1.25911762e-03, -3.16168065e-04, -2.23518698e-04, -7.92510691e-04,\n",
       "        1.21213403e-03, -1.64403080e-03, -1.02338020e-03,  1.05358986e-03,\n",
       "       -1.53894420e-04, -7.12389243e-04,  1.41304312e-03, -9.60393343e-04,\n",
       "        1.93351624e-03, -3.84973711e-04,  1.07292458e-03,  1.83071848e-03,\n",
       "       -1.61370845e-03, -1.46986102e-03, -4.59238538e-04,  5.33444807e-04,\n",
       "        1.54021964e-03,  1.74540328e-04, -5.83867542e-04, -7.66288023e-04,\n",
       "        1.39022782e-03, -2.00469047e-04, -1.17522548e-03, -9.38137760e-04,\n",
       "        1.60136260e-04,  4.66844998e-04,  4.12350520e-04,  1.80831668e-03,\n",
       "        1.82702509e-03,  1.78997777e-03, -7.15863891e-04,  1.17633445e-03,\n",
       "       -1.56372366e-03,  8.38631066e-04,  8.77343118e-04, -1.39105017e-03,\n",
       "       -1.25005050e-03, -1.24410191e-03, -7.93586602e-04,  1.09606958e-03,\n",
       "        5.44026960e-04,  5.05582429e-04, -1.47845596e-03,  1.94846862e-03,\n",
       "        2.01918883e-04,  6.19170489e-04, -1.78220333e-03,  1.86593714e-03,\n",
       "       -1.42881158e-03, -1.33993919e-03,  3.03396955e-04,  1.71039347e-03,\n",
       "       -2.71188794e-04,  1.56455627e-03, -3.36761004e-04, -8.02985742e-04,\n",
       "       -3.53267766e-04, -1.88153773e-03, -1.11726508e-03,  1.74946082e-03,\n",
       "       -5.74279693e-04, -1.34607684e-03, -5.10543585e-04, -1.16493087e-03,\n",
       "       -1.89058308e-03,  1.42959319e-03, -2.75438651e-04, -1.33232470e-03,\n",
       "        1.53734209e-03, -1.27101201e-03,  2.51145102e-05, -1.90563221e-03,\n",
       "       -1.67802267e-03, -1.59976608e-03, -1.61627936e-03, -5.03056217e-04,\n",
       "        1.16083119e-03,  6.47811219e-04,  1.62109267e-04,  3.37862410e-04,\n",
       "        1.60454889e-03,  1.52774807e-03,  4.90362057e-04, -3.49238981e-05,\n",
       "        6.25476008e-04,  1.12902117e-03,  1.54246180e-03,  6.53963070e-05,\n",
       "       -2.49231816e-04,  7.50902342e-04, -4.79005976e-04, -1.43250427e-03,\n",
       "        1.06780301e-03,  1.50669203e-03, -4.48803650e-04, -3.05650174e-04,\n",
       "        1.84535631e-03,  1.72376167e-04, -1.48995523e-03, -1.76066556e-03,\n",
       "       -1.48665870e-03,  4.65654535e-04, -8.65753973e-04, -3.57864192e-04,\n",
       "        1.50553556e-03, -9.29000089e-04,  7.79967289e-04, -4.11542598e-04,\n",
       "        1.35418959e-03, -1.47297338e-03, -1.30489073e-03, -1.78625272e-03,\n",
       "       -1.29301683e-03,  8.38481821e-04,  1.32682105e-03, -1.45649607e-03,\n",
       "       -2.97562452e-04, -1.83596055e-03, -2.87819421e-05, -3.92929534e-04,\n",
       "       -4.34276764e-04,  4.38129064e-05, -2.81900982e-04, -1.49702781e-03,\n",
       "        1.63473515e-03, -5.67008974e-05,  6.63927058e-04,  1.84146618e-03,\n",
       "       -1.35904120e-03, -2.72075180e-04, -1.12282066e-03, -3.88731831e-04,\n",
       "        4.45129815e-04, -1.64881721e-03,  4.19967342e-04, -1.33099197e-03,\n",
       "       -1.83648022e-03,  1.16645824e-04,  1.21425954e-03, -2.36385036e-04,\n",
       "        1.31570501e-03,  8.93291784e-04, -4.14196635e-04,  1.32528553e-03,\n",
       "       -1.16960891e-03, -9.84547194e-04,  3.09634022e-04,  8.25183000e-04,\n",
       "        1.73857762e-03, -5.64742368e-04, -1.64659345e-04, -1.29648973e-03,\n",
       "        1.45483785e-03, -1.50879147e-03,  5.54064289e-04,  6.31494913e-05,\n",
       "        8.55396502e-04,  7.71292951e-04, -4.18062671e-04,  1.69444038e-03,\n",
       "       -1.72147900e-03, -1.70185848e-03, -2.28120829e-04, -1.22689642e-04,\n",
       "       -1.32293766e-03, -1.31372525e-03, -1.37174816e-03, -1.68926630e-03,\n",
       "       -3.47420457e-04,  1.88753428e-03,  1.82721834e-03,  1.38982874e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word.infer_vector(np.asarray(title_abst[0],dtype='str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c51ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec version\n",
    "vect_ta = []\n",
    "for cont in train_data:\n",
    "    titl = cont['title']\n",
    "    abstr = cont['abstract']\n",
    "    all_ta = titl+abstr\n",
    "    vector = np.zeros(256)\n",
    "    for wor in all_ta:\n",
    "        vector +=model_word[wor]\n",
    "    vector = vector/len(all_ta)\n",
    "    vect_ta.append(vector)\n",
    "vect_ta = np.array(vect_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e12ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using one hot method \n",
    "venue_l = []\n",
    "for cont in train_data:\n",
    "    #venue id from 0 to 464, include ' ' will have 466 id\n",
    "    vn_n = np.zeros(466)\n",
    "    ven = cont['venue']\n",
    "    if ven == '':\n",
    "        vn_n[-1] = 1\n",
    "    else:\n",
    "        vn_n[ven] = 1\n",
    "    venue_l.append(vn_n)\n",
    "venue_l = np.array(venue_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2193e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out of distribution \n",
    "#it means that find out the id more than 100 but corparate with less than id's is 100\n",
    "co_auth = {}\n",
    "for cont in train_data:\n",
    "    auth = np.array(cont['authors'])\n",
    "    auth_100 = auth[auth<100]\n",
    "    auth_101 = auth[auth>=100]\n",
    "    if len(auth_100)!=0:\n",
    "        for j in auth_101:\n",
    "            co_auth[j] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d2456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using one hot vector method for co-author\n",
    "co_author = []\n",
    "idx_coau ={}\n",
    "coau_l = list(co_auth.keys())\n",
    "for d,idx in enumerate(coau_l):\n",
    "    idx_coau[idx] = d\n",
    "    \n",
    "\n",
    "for conte in train_data:\n",
    "    auth = np.array(cont['authors'])\n",
    "    auth_100 = auth[auth<100]\n",
    "    auth_101 = auth[auth>=100]\n",
    "    co_n = np.zeros(6576)# len of co-author\n",
    "    if len(auth_100) == 0:\n",
    "        co_n[-1] = 1\n",
    "    else:\n",
    "        for i in auth_101:\n",
    "            co_n[idx_coau[i]] =1\n",
    "    co_author.append(co_n)\n",
    "co_author = np.array(co_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e081cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the array together \n",
    "thr_feat = np.concatenate((vect_ta,venue_l),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32387665",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_featr = np.concatenate((thr_feat ,co_author),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d375df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_doc.npy',mix_featr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0d2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =json.load(open('test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "145bb026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec version\n",
    "vect_test = []\n",
    "for cont in test_data:\n",
    "    titl = cont['title']\n",
    "    abstr = cont['abstract']\n",
    "    all_ta = titl+abstr\n",
    "    vector = np.zeros(256)\n",
    "    for wor in all_ta:\n",
    "        vector +=model_word[wor]\n",
    "    vector = vector/len(all_ta)\n",
    "    vect_test.append(vector)\n",
    "vect_test = np.array(vect_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a765021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using one hot method \n",
    "venue_test = []\n",
    "for cont in test_data:\n",
    "    #venue id from 0 to 464, include ' ' will have 466 id\n",
    "    vn_n = np.zeros(466)\n",
    "    ven = cont['venue']\n",
    "    if ven == '':\n",
    "        vn_n[-1] = 1\n",
    "    else:\n",
    "        vn_n[ven] = 1\n",
    "    venue_test.append(vn_n)\n",
    "venue_test = np.array(venue_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8e6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_author_test = []\n",
    "for conte in test_data:\n",
    "    auth = np.array(cont['coauthors'])\n",
    "    auth_100 = auth[auth<100]\n",
    "    auth_101 = auth[auth>=100]\n",
    "    co_n = np.zeros(6576)# len of co-author\n",
    "    if len(auth_100) == 0:\n",
    "        co_n[-1] = 1\n",
    "    else:\n",
    "        for i in auth_101:\n",
    "            co_n[idx_coau[i]] = 1\n",
    "    co_author_test.append(co_n)\n",
    "co_author_test = np.array(co_author_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cba539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_feat_test = np.concatenate((vect_test,venue_test),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26976285",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_featr_test = np.concatenate((thr_feat_test ,co_author_test),axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65f4318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_word.npy',mix_featr_test)\n",
    "#save the feature data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c229ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification model\n",
    "#using wor2vec to test\n",
    "x =np.load('train_doc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "924c7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_sc = StandardScaler()\n",
    "x = st_sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d02dc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if want to remove the some of the auther id which include after 100\n",
    "idx_mo100 = np.where(author_label_train[:,-1] == 1)[0]\n",
    "idx_un100 = np.where(author_label_train[:,-1] != 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bdc3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_num = x[idx_un100]\n",
    "mo_num = x[idx_mo100]\n",
    "un_l = author_label_train[idx_un100]\n",
    "mo_l = author_label_train[idx_mo100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "559b9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need add some dataset that make the total number of dataset is the close to orginal\n",
    "repeat_time = int((mo_num.shape[0] - un_num.shape[0])/un_num.shape[0])\n",
    "repeat_un = np.repeat(un_num,2,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34b62933",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_label = np.repeat(un_l,2,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa339d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x,repeat_un))\n",
    "label_repeat = np.concatenate((author_label_train,repeat_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "905d0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after add dataset \n",
    "x_train,x_test,y_train,y_test = train_test_split(x,label_repeat,test_size = 0.3,random_state =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f6645fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 20:22:42.206281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "223/223 [==============================] - 20s 85ms/step - loss: 0.1414\n",
      "Epoch 2/150\n",
      "223/223 [==============================] - 19s 85ms/step - loss: 0.0485\n",
      "Epoch 3/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0464\n",
      "Epoch 4/150\n",
      "223/223 [==============================] - 21s 96ms/step - loss: 0.0452\n",
      "Epoch 5/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0443\n",
      "Epoch 6/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0433\n",
      "Epoch 7/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0422\n",
      "Epoch 8/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0431\n",
      "Epoch 9/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0407\n",
      "Epoch 10/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0396\n",
      "Epoch 11/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0388\n",
      "Epoch 12/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0381\n",
      "Epoch 13/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0375\n",
      "Epoch 14/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0407\n",
      "Epoch 15/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0534\n",
      "Epoch 16/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0375\n",
      "Epoch 17/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0364\n",
      "Epoch 18/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0357\n",
      "Epoch 19/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0352\n",
      "Epoch 20/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0346\n",
      "Epoch 21/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0340\n",
      "Epoch 22/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0334\n",
      "Epoch 23/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0330\n",
      "Epoch 24/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0325\n",
      "Epoch 25/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0320\n",
      "Epoch 26/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0316\n",
      "Epoch 27/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0337\n",
      "Epoch 28/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0318\n",
      "Epoch 29/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0309\n",
      "Epoch 30/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0303\n",
      "Epoch 31/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0299\n",
      "Epoch 32/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0293\n",
      "Epoch 33/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0288\n",
      "Epoch 34/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0283\n",
      "Epoch 35/150\n",
      "223/223 [==============================] - 21s 96ms/step - loss: 0.0278\n",
      "Epoch 36/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0274\n",
      "Epoch 37/150\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0269\n",
      "Epoch 38/150\n",
      "223/223 [==============================] - 22s 98ms/step - loss: 0.0264\n",
      "Epoch 39/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0259\n",
      "Epoch 40/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0256\n",
      "Epoch 41/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0252\n",
      "Epoch 42/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0246\n",
      "Epoch 43/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0241\n",
      "Epoch 44/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0238\n",
      "Epoch 45/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0233\n",
      "Epoch 46/150\n",
      "223/223 [==============================] - 21s 96ms/step - loss: 0.0229\n",
      "Epoch 47/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0225\n",
      "Epoch 48/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0222\n",
      "Epoch 49/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0216\n",
      "Epoch 50/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0212\n",
      "Epoch 51/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0209\n",
      "Epoch 52/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0205\n",
      "Epoch 53/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0199\n",
      "Epoch 54/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0197\n",
      "Epoch 55/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0193\n",
      "Epoch 56/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0190\n",
      "Epoch 57/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0187\n",
      "Epoch 58/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0183\n",
      "Epoch 59/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0180\n",
      "Epoch 60/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0177\n",
      "Epoch 61/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0175\n",
      "Epoch 62/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0171\n",
      "Epoch 63/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0169\n",
      "Epoch 64/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0174\n",
      "Epoch 65/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0165\n",
      "Epoch 66/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0160\n",
      "Epoch 67/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0158\n",
      "Epoch 68/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0154\n",
      "Epoch 69/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0151\n",
      "Epoch 70/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0150\n",
      "Epoch 71/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0147\n",
      "Epoch 72/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0147\n",
      "Epoch 73/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0145\n",
      "Epoch 74/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0142\n",
      "Epoch 75/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0139\n",
      "Epoch 76/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0137\n",
      "Epoch 77/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0133\n",
      "Epoch 78/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0134\n",
      "Epoch 79/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0151\n",
      "Epoch 80/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0139\n",
      "Epoch 81/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0132\n",
      "Epoch 82/150\n",
      "223/223 [==============================] - 22s 99ms/step - loss: 0.0128\n",
      "Epoch 83/150\n",
      "223/223 [==============================] - 24s 108ms/step - loss: 0.0126\n",
      "Epoch 84/150\n",
      "223/223 [==============================] - 21s 96ms/step - loss: 0.0123\n",
      "Epoch 85/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0123\n",
      "Epoch 86/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0121\n",
      "Epoch 87/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0120\n",
      "Epoch 88/150\n",
      "223/223 [==============================] - 22s 97ms/step - loss: 0.0117\n",
      "Epoch 89/150\n",
      "223/223 [==============================] - 22s 97ms/step - loss: 0.0117\n",
      "Epoch 90/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0114\n",
      "Epoch 91/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0114\n",
      "Epoch 92/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0113\n",
      "Epoch 93/150\n",
      "223/223 [==============================] - 22s 97ms/step - loss: 0.0110\n",
      "Epoch 94/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0109\n",
      "Epoch 95/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0109\n",
      "Epoch 96/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0107\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0105\n",
      "Epoch 98/150\n",
      "223/223 [==============================] - 20s 89ms/step - loss: 0.0103\n",
      "Epoch 99/150\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0105\n",
      "Epoch 100/150\n",
      "223/223 [==============================] - 20s 89ms/step - loss: 0.0102\n",
      "Epoch 101/150\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0101\n",
      "Epoch 102/150\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0098\n",
      "Epoch 103/150\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0099\n",
      "Epoch 104/150\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0096\n",
      "Epoch 105/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0095\n",
      "Epoch 106/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0094\n",
      "Epoch 107/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0095\n",
      "Epoch 108/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0092\n",
      "Epoch 109/150\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0093\n",
      "Epoch 110/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0091\n",
      "Epoch 111/150\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.0089\n",
      "Epoch 112/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0088\n",
      "Epoch 113/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0087\n",
      "Epoch 114/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0086\n",
      "Epoch 115/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0085\n",
      "Epoch 116/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0084\n",
      "Epoch 117/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0083\n",
      "Epoch 118/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0084\n",
      "Epoch 119/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0081\n",
      "Epoch 120/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0081\n",
      "Epoch 121/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0081\n",
      "Epoch 122/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0080\n",
      "Epoch 123/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0079\n",
      "Epoch 124/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0079\n",
      "Epoch 125/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0079\n",
      "Epoch 126/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0076\n",
      "Epoch 127/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0077\n",
      "Epoch 128/150\n",
      "223/223 [==============================] - 20s 91ms/step - loss: 0.0075\n",
      "Epoch 129/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0074\n",
      "Epoch 130/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0072\n",
      "Epoch 131/150\n",
      "223/223 [==============================] - 20s 92ms/step - loss: 0.0073\n",
      "Epoch 132/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0073\n",
      "Epoch 133/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0072\n",
      "Epoch 134/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0073\n",
      "Epoch 135/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0071\n",
      "Epoch 136/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0069\n",
      "Epoch 137/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0067\n",
      "Epoch 138/150\n",
      "223/223 [==============================] - 21s 96ms/step - loss: 0.0067\n",
      "Epoch 139/150\n",
      "223/223 [==============================] - 21s 92ms/step - loss: 0.0068\n",
      "Epoch 140/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0066\n",
      "Epoch 141/150\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.0066\n",
      "Epoch 142/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0065\n",
      "Epoch 143/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0065\n",
      "Epoch 144/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0063\n",
      "Epoch 145/150\n",
      "223/223 [==============================] - 23s 103ms/step - loss: 0.0062\n",
      "Epoch 146/150\n",
      "223/223 [==============================] - 24s 106ms/step - loss: 0.0063\n",
      "Epoch 147/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0063\n",
      "Epoch 148/150\n",
      "223/223 [==============================] - 21s 95ms/step - loss: 0.0068\n",
      "Epoch 149/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0064\n",
      "Epoch 150/150\n",
      "223/223 [==============================] - 21s 94ms/step - loss: 0.0062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f815e1ce310>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = PolynomialDecay(initial_learning_rate = 0.001,decay_steps = 150,end_learning_rate = 0.00025,power = 0.5)\n",
    "model_all = Sequential()\n",
    "model_all.add(Dense(2048, input_dim = x_train.shape[1],activation= 'tanh'))\n",
    "model_all.add(Dropout(0.3))\n",
    "model_all.add(Dense(512,activation ='tanh'))\n",
    "model_all.add(Dropout(0.3))\n",
    "model_all.add(Dense(512,activation ='tanh'))\n",
    "model_all.add(Dropout(0.3))\n",
    "model_all.add(Dense(256,activation ='tanh'))\n",
    "model_all.add(Dropout(0.3))\n",
    "model_all.add(Dense(y_train.shape[1],activation ='sigmoid'))\n",
    "model_all.compile(loss = 'binary_crossentropy',optimizer=Adam(learning_rate))\n",
    "model_all.fit(x_train,y_train,epochs = 150,batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52246c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891/891 [==============================] - 9s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "predi_re = model_all.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e533a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "predi_re = np.where(predi_re >=0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14f0285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9962193905139272"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true = y_train, y_pred = predi_re,average ='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfd98c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 4s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8625314681722602"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predi_test = model_all.predict(x_test)\n",
    "predi_test = np.where(predi_test >=0.5,1,0)\n",
    "f1_score(y_true = y_test, y_pred = predi_test,average ='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d85c6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load('test_word.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dae0268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = st_sc.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c869202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "predic_final = model_all.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26f76205",
   "metadata": {},
   "outputs": [],
   "source": [
    "predic_final_idx = [np.where(i >=0.5) for i in predic_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88b1bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_t = pd.DataFrame(predic_final_idx, columns =['Predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a760974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predict\n",
       "0     [100]\n",
       "1     [100]\n",
       "2     [100]\n",
       "3     [100]\n",
       "4     [100]\n",
       "..      ...\n",
       "795   [100]\n",
       "796      []\n",
       "797   [100]\n",
       "798   [100]\n",
       "799      []\n",
       "\n",
       "[800 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475eb7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_t.to_csv('predict.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
